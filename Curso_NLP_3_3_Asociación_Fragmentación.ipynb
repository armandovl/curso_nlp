{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curso NLP 3.3 Asociaci√≥n Fragmentaci√≥n",
      "provenance": [],
      "collapsed_sections": [
        "iskIFXoKOHWJ",
        "CbnHNtDZwtJJ",
        "-jMcxMRqO_JB",
        "Mh8Bw-ueLCUh",
        "gzvZhG8nhpQV",
        "gWmEeftbAOBY",
        "5HcSWlhla33C",
        "urRfQhrLF_OM",
        "XM2WfIPJGLVS",
        "J-xSGpDNHBq6",
        "52H0poTQHLMe",
        "EfkIA8ghJ7eq",
        "WWYu6JD6KDZ6",
        "ISrOXSo7KZQM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandovl/curso_nlp/blob/main/Curso_NLP_3_3_Asociaci%C3%B3n_Fragmentaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kube0b4oHem7"
      },
      "source": [
        "<img src=\"https://i1.wp.com/www.sopitas.com/wp-content/uploads/2014/08/Tec-de-Monterrey-Logo-640x581.jpg\" width=\"70\"/> \n",
        "<h5>ITESM Campus Santa Fe <br> Armando Vald√©s </h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U39mK1OlJ4mR"
      },
      "source": [
        "### Nomenclatura\n",
        "üü† Modificar <br>\n",
        "‚èÆ Repetir en caso de cambiar texto<br>\n",
        "üîò Opcional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iskIFXoKOHWJ"
      },
      "source": [
        "### **1.-Importar el archivo** üü† "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aquh7m6VKH2A"
      },
      "source": [
        "#1#\n",
        "#lo primero que hacemos es importar el archivo de texto, en r de reescribir\n",
        "archivo=open(\"estrellas_bueno_soluble.txt\",\"r\")\n",
        "texto_del_archivo=archivo.read() #el texto del txt se queda en la variable texto_del_archivo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbnHNtDZwtJJ"
      },
      "source": [
        "### **2.-Importar bibliotecas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jMcxMRqO_JB"
      },
      "source": [
        "#### Cargar NLTK <br>\n",
        "TOKENIZACION, ELIMINAR STOP WORDS, DERIVACI√ìN <BR>\n",
        "GRAFICAR Y EXPORTAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anIBKo8TO-A_"
      },
      "source": [
        "#2#\n",
        "import nltk #importamos nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQh_337xP-xH"
      },
      "source": [
        "#3#\n",
        "nltk.download('punkt') #parte para tokenizar\n",
        "nltk.download('stopwords') #parte para stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh8Bw-ueLCUh"
      },
      "source": [
        "#### Cargar SPACY <br>\n",
        "VERBOS, ADJETIVOS,LEMATIZACION, SUSTANTIVOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEbPb4BOOXuC"
      },
      "source": [
        "#4#\n",
        "#!pip install spacy #NO ES NECESARIO PERO POR SI SE TIENE QUE INSTALAR\n",
        "import spacy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERpB0BWPCP-"
      },
      "source": [
        "#5#\n",
        "#Descargar librer√≠a\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7JVQg4bPORU"
      },
      "source": [
        "#6#\n",
        "#cargar libreria espa√±ol\n",
        "#SI NO CARGA HAY QUE EJECUTAR TODO DE NUEVO\n",
        "nlp= spacy.load('es_core_news_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjfUw-PrRnM"
      },
      "source": [
        "#7# una vez que se ejecute todo se comenta este c√≥digo\n",
        "print(detener) #la uso para detener el ejecutar todo de nuevo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzvZhG8nhpQV"
      },
      "source": [
        "### **3.- Hacemos las funciones previas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaOECbmjh_7J"
      },
      "source": [
        "#Creamos funci√≥n lematizar\n",
        "def lematizar(texto_a_lematizar):\n",
        "  texto_lematizado = []\n",
        "  separator = ' '\n",
        "  for token in nlp(separator.join(texto_a_lematizar)): #Se juntan los tokens en una string, string es el formato necesario para nlp\n",
        "    #print(token.text, token.lemma_, token.pos_)\n",
        "    texto_lematizado.append(token.lemma_)\n",
        "  return(texto_lematizado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXUO6GWDiEEH"
      },
      "source": [
        "#Creamos funci√≥n derivaci√≥n regresiva\n",
        "def derivar_texto(texto_a_derivar):\n",
        "  salida_texto_derivado=[]\n",
        "  from nltk.stem import SnowballStemmer\n",
        "  derivar=SnowballStemmer(\"spanish\")\n",
        "  for derv in texto_a_derivar:\n",
        "    salida_texto_derivado.append(derivar.stem(derv))\n",
        "  return(salida_texto_derivado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWmEeftbAOBY"
      },
      "source": [
        "### **4.-Aplicar Spacy** ‚èÆ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNGlmZn7dCWh"
      },
      "source": [
        "doc = nlp(texto_del_archivo) #aplicamos spacy a la librer√≠a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HcSWlhla33C"
      },
      "source": [
        "### **5.-Fragmentacion (principios)** <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urRfQhrLF_OM"
      },
      "source": [
        "#### Ejemplo texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVmGGC3ZELZm"
      },
      "source": [
        "![imagen Ch√©rnobil](https://miro.medium.com/max/1400/1*JMaoWo2995GuQsM5ZA3-gg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM2WfIPJGLVS"
      },
      "source": [
        " #### Texto ejemplo fragmentado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16WmGvj5Emhx"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/716/1*5xgwg5ohFFhBOD-3WVBcVw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-xSGpDNHBq6"
      },
      "source": [
        "#### Ejemplos simples del texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U60wOZzRa7KH"
      },
      "source": [
        "'''\n",
        "#ejemplo de la fragmentacion en el documento\n",
        "\n",
        "#devuelve fragmentados\n",
        "for palabra in doc.noun_chunks:\n",
        "    print(palabra.root.text,'es conector de ', palabra.root.head.text)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOfNHBnyHGkF"
      },
      "source": [
        "'''\n",
        "#ejemplo volteado de fragmentar y unirlos con un separador\n",
        "#fragmentados  sin lematizar\n",
        "fragmentaciones_sin_le=[]\n",
        "for palabra in doc.noun_chunks:\n",
        "    separador = 'es conector de'\n",
        "    #print(palabra.root.head.text,'#',palabra.root.text)\n",
        "    fragmentaciones_sin_le.append(separador.join([palabra.root.text,palabra.root.head.text]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52H0poTQHLMe"
      },
      "source": [
        "### **6.-Manos a la obra** ‚èÆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfkIA8ghJ7eq"
      },
      "source": [
        "#### Aplicamos la fragmentaci√≥n al texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "925WuPZW5RS-"
      },
      "source": [
        "#hacemos las conexiones y los guardamos en dos arreglos\n",
        "fragmentaciones1=[]\n",
        "fragmentaciones2=[]\n",
        "for palabra in doc.noun_chunks:\n",
        "    separador = ' '\n",
        "    #print(palabra.root.text,'#', palabra.root.head.text)\n",
        "    fragmentaciones1.append(palabra.root.text)\n",
        "    fragmentaciones2.append(palabra.root.head.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWYu6JD6KDZ6"
      },
      "source": [
        "#### Aplicamos lematizaci√≥n y/o derivaci√≥n regresiva"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP9r22s8B3x-"
      },
      "source": [
        "#hacemos la lematizacion de los dos arreglos aqu√¨ cambiamos el √≥rden\n",
        "para_meta=lematizar(fragmentaciones1)\n",
        "de_origen=lematizar(fragmentaciones2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ7WvUt8yLOk"
      },
      "source": [
        "###################### Opcional voy a gregar  la derivacion##########################\n",
        "para_meta=derivar_texto(para_meta)\n",
        "de_origen=derivar_texto(de_origen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1VqcKHavSDk"
      },
      "source": [
        "#ver como queda la lematizaci√≥n y/o derivaci√≥n\n",
        "'''\n",
        "contador=0\n",
        "for a in para_meta:\n",
        "  print(fragmentaciones1[contador],\" # \",para_meta[contador])\n",
        "  contador=contador+1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISrOXSo7KZQM"
      },
      "source": [
        "#### Lo procesamos para exportar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X19aUv5XCWCl"
      },
      "source": [
        "  #lo transformamos en un dataframe\n",
        "  import pandas as pd #importamos la biblioteca\n",
        "  dataframe_fragmentados=pd.DataFrame(de_origen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXu7sddIsjpO"
      },
      "source": [
        " dataframe_fragmentados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOCpE18GDpX4"
      },
      "source": [
        "#Agregamos la segunda columna\n",
        "dataframe_fragmentados['para_meta'] = para_meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAnPJ5DkFQUP"
      },
      "source": [
        "dataframe_fragmentados #as√≠ queda el data frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52nh1DFqDjpy"
      },
      "source": [
        "  #exportamos a csv\n",
        "  dataframe_fragmentados.columns=[\"Source\",\"Target\"] #agregamos nombre a las columnas\n",
        "  dataframe_fragmentados.to_csv(\"vertices_bueno_soluble.csv\", index=False, encoding='utf-8') #exportamos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnfhwiywGOkn"
      },
      "source": [
        "#### Proceso post exportaci√≥n\n",
        "1.-Pasar el archivo a formato ansi con bloc de notas <br>\n",
        "2.-Quitar caracteres extra√±os <br>\n",
        "3.-Pasar todo a min√∫sculas <br>\n",
        "4.-Quitar acentos y √± <br>\n",
        "5.-Nodo archivo **Id Label** <br>\n",
        "6.-Aristas archivo **Source Target** <br>\n",
        "3.-Se analiza en gephi <br>"
      ]
    }
  ]
}